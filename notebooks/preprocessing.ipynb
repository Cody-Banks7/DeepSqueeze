{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Specific Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor\n",
    "\n",
    "[Monitor benchmark dataset](https://github.com/crottyan/mgbench) is used for benchmarking mostly database engines MySQL index creation. As a result the dataset is huge (~7GB) exceeding my hardware limits.\n",
    "\n",
    "* Total remaining rows: 2,202,375\n",
    "* Columns: 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = pd.read_csv('../storage/datasets/monitor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu_idle         float64\n",
      "cpu_nice         float64\n",
      "cpu_system       float64\n",
      "cpu_user         float64\n",
      "cpu_wio          float64\n",
      "disk_free        float64\n",
      "disk_total       float64\n",
      "part_max_used    float64\n",
      "load_fifteen     float64\n",
      "load_five        float64\n",
      "load_one         float64\n",
      "mem_buffers      float64\n",
      "mem_cached       float64\n",
      "mem_free         float64\n",
      "mem_shared       float64\n",
      "swap_free        float64\n",
      "bytes_in         float64\n",
      "bytes_out        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Keep only 30% of the rows\n",
    "monitor = monitor.sample(frac=1).iloc[:int(monitor.shape[0]*0.2), :]\n",
    "\n",
    "# Drop the categorical columns\n",
    "monitor = monitor.drop(['log_time', 'machine_name', 'machine_group'], axis=1)\n",
    "\n",
    "# Drop remaining NaN rows\n",
    "monitor = monitor.dropna()\n",
    "\n",
    "# Make sure that we keep only numerical (float) types\n",
    "print(monitor.dtypes)\n",
    "\n",
    "# Store it without the default pandas index and without the column names\n",
    "monitor.to_csv('../storage/datasets/monitor_processed_0_2_fraction.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berkeley\n",
    "\n",
    "[Berkeley](https://www.kaggle.com/divyansh22/intel-berkeley-research-lab-sensor-data) contains data from 51 environmentanl sensors in the Berkeley Research lab between February 28th and April 5th, 2004.\n",
    "\n",
    "* Total rows: 2,219,803\n",
    "* Columns: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley = pd.read_csv('../storage/datasets/berkeley.csv', sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NaNs and drop them (~4% of the dataset)\n",
    "berkeley = berkeley.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform date column from yyyy-mm-dd to days from the start of the measurements (28/2/2014)\n",
    "year_dates = pd.to_datetime(berkeley.iloc[:, 0], format='%Y/%m/%d')\n",
    "basedate = pd.Timestamp('2004-2-28')\n",
    "days_elapsed = year_dates.apply(lambda x: (x - basedate).days)\n",
    "berkeley[0] = days_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform hour column from hh-mm-ss to days from the start of the measurements (28/2/2014)\n",
    "hour = pd.to_datetime(berkeley.iloc[:, 1])\n",
    "minutes_elapsed = ((hour - hour.dt.normalize()) / pd.Timedelta('1 minute')).astype(int)\n",
    "berkeley[1] = minutes_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store it without the default pandas index and without the column names\n",
    "berkeley.to_csv('../storage/datasets/berkeley_processed.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corel\n",
    "\n",
    "[Corel histogram dataset](https://kdd.ics.uci.edu/databases/CorelFeatures/CorelFeatures.data.html) consists of histogram color values of 68,040 photo images from various categories.\n",
    "\n",
    "\n",
    "* Total rows: 68,040\n",
    "* Columns: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corel = pd.read_csv('../storage/datasets/corel.csv', sep=' ', header=None, index_col=0)\n",
    "\n",
    "# Corel does not require any dataset-specific preprocessing, we just remove its index when we store it\n",
    "corel.to_csv('../storage/datasets/corel_preprocessed.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
